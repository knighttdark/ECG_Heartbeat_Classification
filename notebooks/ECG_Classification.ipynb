{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q_GPTK0ODefD",
        "outputId": "e952cfd9-e46c-4d52-9fa3-2c425ad4b14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wJ0wgY-D0Wg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/ECG_Heartbeat_Classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import các thư viện cần thiết\n",
        "\n",
        "Trong phần này, ta import các thư viện cơ bản để xử lý dữ liệu, xây dựng và huấn luyện mô hình học sâu (CNN) cho phân loại tín hiệu ECG.\n"
      ],
      "metadata": {
        "id": "zFkyHYHQ-VqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.3 pandas==2.2.2 scipy==1.13.1 plotly==5.18.0 tensorflow==2.12.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "brjKV4m3C-Si",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57185950-a28e-46b5-a60d-4cca1dcd8764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.3 in /usr/local/lib/python3.11/dist-packages (1.23.3)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: plotly==5.18.0 in /usr/local/lib/python3.11/dist-packages (5.18.0)\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly==5.18.0) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly==5.18.0) (24.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.1.3"
      ],
      "metadata": {
        "id": "ewgZ9JDIs069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d163eb9-83a1-4c8d-fa97-efa04416aee1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (3.6.0)\n",
            "Downloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.0/32.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tiền xử lý dữ liệu MIT-BIH (Arrhythmia Classification)\n",
        "\n",
        "Đọc dữ liệu nhịp tim từ MIT-BIH, shuffle để tránh bias, và reshape lại đầu vào cho phù hợp với Conv1D. Nhãn là các giá trị từ 0 đến 4 tương ứng với 5 loại nhịp tim theo chuẩn AAMI EC57.\n"
      ],
      "metadata": {
        "id": "hfkzgT5K-iJO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lcIBhNvbfKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba81d373-68f7-413d-83b1-b0cfd9b1cb6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21892, 188)\n",
            "Counter({0.0: 18118, 4.0: 1608, 2.0: 1448, 1.0: 556, 3.0: 162})\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from plotly.offline import plot\n",
        "import plotly.graph_objs as go\n",
        "from collections import Counter\n",
        "\n",
        "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "image_dir = os.path.join(base_dir, \"images\")\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(\"./input/mitbih_test.csv\", header=None)\n",
        "print(df.shape)\n",
        "print(Counter(df[187].values))\n",
        "\n",
        "Y = np.array(df[187].values).astype(np.int8)\n",
        "X = np.array(df[list(range(187))].values)\n",
        "\n",
        "indexes = random.sample(list(range(df.shape[0])), 10)\n",
        "\n",
        "for i in indexes:\n",
        "    data = [go.Scatter(\n",
        "              x=list(range(187)),\n",
        "              y=X[i, :])]\n",
        "\n",
        "    plot({\"data\": data,\n",
        "          \"layout\": {\"title\": \"Heartbeat Class : %s \" % Y[i]}}, filename=os.path.join(image_dir, f\"{i}.html\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Huấn luyện mạng CNN để phân loại 5 loại nhịp tim từ tập dữ liệu MIT-BIH\n",
        "Huấn luyện một mạng nơ-ron tích chập 1 chiều (1D-CNN) để phân loại các loại nhịp tim bất thường từ tập dữ liệu MIT-BIH Arrhythmia."
      ],
      "metadata": {
        "id": "A004-3x4527r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import optimizers, losses, activations, models\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(\"./input/mitbih_train.csv\", header=None)\n",
        "df_train = df_train.sample(frac=1)\n",
        "df_test = pd.read_csv(\"./input/mitbih_test.csv\", header=None)\n",
        "\n",
        "Y = np.array(df_train[187].values).astype(np.int8)\n",
        "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
        "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    nclass = 5\n",
        "    #inp = Input(batch_shape=(1, 187, 1))\n",
        "    inp = Input(shape=(187, 1))\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = GlobalMaxPool1D()(img_1)\n",
        "    img_1 = Dropout(rate=0.2)(img_1)\n",
        "\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
        "    dense_1 = Dense(nclass, activation=activations.softmax, name=\"dense_3_mitbih\")(dense_1)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=dense_1)\n",
        "    opt = optimizers.Adam(0.001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "file_path = \"./models/train_mitbih_cnn.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
        "callbacks_list = [checkpoint, early, redonplat]  # early\n",
        "\n",
        "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
        "model.load_weights(file_path)\n",
        "\n",
        "pred_test = model.predict(X_test)\n",
        "pred_test = np.argmax(pred_test, axis=-1)\n",
        "\n",
        "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
        "\n",
        "print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test accuracy score : %s \"% acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMHmYMVIqDyy",
        "outputId": "5c520fb4-8372-414a-b025-7911a5d31ce0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 187, 1)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 183, 16)           96        \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 179, 16)           1296      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 89, 16)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 89, 16)            0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 87, 32)            1568      \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 85, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 42, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 42, 32)            0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 40, 32)            3104      \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 38, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 19, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 19, 32)            0         \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 17, 256)           24832     \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 15, 256)           196864    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3_mitbih (Dense)      (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 254,901\n",
            "Trainable params: 254,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_acc improved from -inf to 0.94632, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 103s - loss: 0.3389 - acc: 0.9013 - val_loss: 0.1835 - val_acc: 0.9463 - lr: 0.0010 - 103s/epoch - 42ms/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_acc improved from 0.94632 to 0.96894, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 109s - loss: 0.1889 - acc: 0.9486 - val_loss: 0.1210 - val_acc: 0.9689 - lr: 0.0010 - 109s/epoch - 44ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_acc improved from 0.96894 to 0.97179, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 101s - loss: 0.1460 - acc: 0.9606 - val_loss: 0.1063 - val_acc: 0.9718 - lr: 0.0010 - 101s/epoch - 41ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_acc improved from 0.97179 to 0.97270, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 106s - loss: 0.1285 - acc: 0.9653 - val_loss: 0.1064 - val_acc: 0.9727 - lr: 0.0010 - 106s/epoch - 43ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_acc improved from 0.97270 to 0.97670, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 104s - loss: 0.1181 - acc: 0.9681 - val_loss: 0.1009 - val_acc: 0.9767 - lr: 0.0010 - 104s/epoch - 42ms/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_acc improved from 0.97670 to 0.97819, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 100s - loss: 0.1088 - acc: 0.9706 - val_loss: 0.0850 - val_acc: 0.9782 - lr: 0.0010 - 100s/epoch - 40ms/step\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 7: val_acc improved from 0.97819 to 0.97841, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 99s - loss: 0.1007 - acc: 0.9723 - val_loss: 0.0829 - val_acc: 0.9784 - lr: 0.0010 - 99s/epoch - 40ms/step\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 8: val_acc improved from 0.97841 to 0.98036, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 98s - loss: 0.0959 - acc: 0.9737 - val_loss: 0.0756 - val_acc: 0.9804 - lr: 0.0010 - 98s/epoch - 40ms/step\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.98036\n",
            "2463/2463 - 98s - loss: 0.0904 - acc: 0.9745 - val_loss: 0.0743 - val_acc: 0.9804 - lr: 0.0010 - 98s/epoch - 40ms/step\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.98036\n",
            "2463/2463 - 98s - loss: 0.0872 - acc: 0.9752 - val_loss: 0.0690 - val_acc: 0.9802 - lr: 0.0010 - 98s/epoch - 40ms/step\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 11: val_acc improved from 0.98036 to 0.98275, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 99s - loss: 0.0806 - acc: 0.9771 - val_loss: 0.0655 - val_acc: 0.9828 - lr: 0.0010 - 99s/epoch - 40ms/step\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 12: val_acc did not improve from 0.98275\n",
            "2463/2463 - 96s - loss: 0.0781 - acc: 0.9782 - val_loss: 0.0642 - val_acc: 0.9817 - lr: 0.0010 - 96s/epoch - 39ms/step\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.98275\n",
            "2463/2463 - 99s - loss: 0.0772 - acc: 0.9779 - val_loss: 0.0691 - val_acc: 0.9818 - lr: 0.0010 - 99s/epoch - 40ms/step\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.98275\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "2463/2463 - 99s - loss: 0.0735 - acc: 0.9788 - val_loss: 0.0652 - val_acc: 0.9825 - lr: 0.0010 - 99s/epoch - 40ms/step\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 15: val_acc improved from 0.98275 to 0.98641, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 100s - loss: 0.0542 - acc: 0.9840 - val_loss: 0.0499 - val_acc: 0.9864 - lr: 1.0000e-04 - 100s/epoch - 41ms/step\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 16: val_acc improved from 0.98641 to 0.98721, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 104s - loss: 0.0490 - acc: 0.9854 - val_loss: 0.0479 - val_acc: 0.9872 - lr: 1.0000e-04 - 104s/epoch - 42ms/step\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 17: val_acc improved from 0.98721 to 0.98812, saving model to ./models/train_mitbih_cnn.h5\n",
            "2463/2463 - 107s - loss: 0.0467 - acc: 0.9862 - val_loss: 0.0467 - val_acc: 0.9881 - lr: 1.0000e-04 - 107s/epoch - 44ms/step\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.98812\n",
            "2463/2463 - 103s - loss: 0.0444 - acc: 0.9865 - val_loss: 0.0466 - val_acc: 0.9876 - lr: 1.0000e-04 - 103s/epoch - 42ms/step\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.98812\n",
            "2463/2463 - 103s - loss: 0.0436 - acc: 0.9870 - val_loss: 0.0461 - val_acc: 0.9881 - lr: 1.0000e-04 - 103s/epoch - 42ms/step\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.98812\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "2463/2463 - 103s - loss: 0.0427 - acc: 0.9873 - val_loss: 0.0457 - val_acc: 0.9871 - lr: 1.0000e-04 - 103s/epoch - 42ms/step\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 21: val_acc did not improve from 0.98812\n",
            "2463/2463 - 100s - loss: 0.0402 - acc: 0.9879 - val_loss: 0.0453 - val_acc: 0.9879 - lr: 1.0000e-05 - 100s/epoch - 41ms/step\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 22: val_acc did not improve from 0.98812\n",
            "2463/2463 - 102s - loss: 0.0404 - acc: 0.9878 - val_loss: 0.0451 - val_acc: 0.9879 - lr: 1.0000e-05 - 102s/epoch - 41ms/step\n",
            "Epoch 22: early stopping\n",
            "685/685 [==============================] - 5s 7ms/step\n",
            "Test f1 score : 0.908807867179134 \n",
            "Test accuracy score : 0.9837383519093733 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Huấn luyện mô hình CNN để phát hiện nhồi máu cơ tim từ dữ liệu PTB-DB\n",
        "Huấn luyện một mạng CNN để phân loại tín hiệu ECG bình thường và bất thường (do nhồi máu cơ tim - MI) từ tập dữ liệu PTB-DB.\n",
        "\n"
      ],
      "metadata": {
        "id": "EAYCu_Yr55QF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK3jF_z1bhl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b984c3-07e2-4fa6-a96f-1329df4c6663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 187, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 183, 16)           96        \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 179, 16)           1296      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 89, 16)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 89, 16)            0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 87, 32)            1568      \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 85, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 42, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 42, 32)            0         \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 40, 32)            3104      \n",
            "                                                                 \n",
            " conv1d_13 (Conv1D)          (None, 38, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 19, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 19, 32)            0         \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 17, 256)           24832     \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 15, 256)           196864    \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3_ptbdb (Dense)       (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 254,641\n",
            "Trainable params: 254,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_acc improved from -inf to 0.80687, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 26s - loss: 0.5039 - acc: 0.7475 - val_loss: 0.4117 - val_acc: 0.8069 - lr: 0.0010 - 26s/epoch - 80ms/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_acc improved from 0.80687 to 0.89185, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 22s - loss: 0.3436 - acc: 0.8547 - val_loss: 0.2596 - val_acc: 0.8918 - lr: 0.0010 - 22s/epoch - 67ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.89185\n",
            "328/328 - 17s - loss: 0.2772 - acc: 0.8857 - val_loss: 0.3479 - val_acc: 0.8618 - lr: 0.0010 - 17s/epoch - 53ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_acc improved from 0.89185 to 0.91416, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.2385 - acc: 0.9066 - val_loss: 0.1973 - val_acc: 0.9142 - lr: 0.0010 - 18s/epoch - 55ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_acc improved from 0.91416 to 0.93476, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 19s - loss: 0.1988 - acc: 0.9232 - val_loss: 0.1682 - val_acc: 0.9348 - lr: 0.0010 - 19s/epoch - 59ms/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_acc improved from 0.93476 to 0.95021, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.1830 - acc: 0.9338 - val_loss: 0.1305 - val_acc: 0.9502 - lr: 0.0010 - 18s/epoch - 56ms/step\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.95021\n",
            "328/328 - 17s - loss: 0.1687 - acc: 0.9366 - val_loss: 0.2304 - val_acc: 0.9039 - lr: 0.0010 - 17s/epoch - 53ms/step\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 8: val_acc did not improve from 0.95021\n",
            "328/328 - 18s - loss: 0.1534 - acc: 0.9436 - val_loss: 0.1931 - val_acc: 0.9288 - lr: 0.0010 - 18s/epoch - 53ms/step\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 9: val_acc improved from 0.95021 to 0.96137, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 19s - loss: 0.1267 - acc: 0.9536 - val_loss: 0.1101 - val_acc: 0.9614 - lr: 0.0010 - 19s/epoch - 59ms/step\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.96137\n",
            "328/328 - 17s - loss: 0.1193 - acc: 0.9556 - val_loss: 0.1178 - val_acc: 0.9536 - lr: 0.0010 - 17s/epoch - 53ms/step\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 11: val_acc improved from 0.96137 to 0.96223, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.1096 - acc: 0.9614 - val_loss: 0.0942 - val_acc: 0.9622 - lr: 0.0010 - 18s/epoch - 53ms/step\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 12: val_acc improved from 0.96223 to 0.96395, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.1012 - acc: 0.9641 - val_loss: 0.1038 - val_acc: 0.9639 - lr: 0.0010 - 18s/epoch - 54ms/step\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.96395\n",
            "328/328 - 19s - loss: 0.1018 - acc: 0.9625 - val_loss: 0.1227 - val_acc: 0.9554 - lr: 0.0010 - 19s/epoch - 57ms/step\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.96395\n",
            "328/328 - 17s - loss: 0.0797 - acc: 0.9727 - val_loss: 0.1463 - val_acc: 0.9459 - lr: 0.0010 - 17s/epoch - 52ms/step\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 15: val_acc improved from 0.96395 to 0.97339, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.0884 - acc: 0.9693 - val_loss: 0.0717 - val_acc: 0.9734 - lr: 0.0010 - 18s/epoch - 55ms/step\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 16: val_acc did not improve from 0.97339\n",
            "328/328 - 18s - loss: 0.0749 - acc: 0.9730 - val_loss: 0.0826 - val_acc: 0.9691 - lr: 0.0010 - 18s/epoch - 55ms/step\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 17: val_acc improved from 0.97339 to 0.97940, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 19s - loss: 0.0690 - acc: 0.9766 - val_loss: 0.0723 - val_acc: 0.9794 - lr: 0.0010 - 19s/epoch - 59ms/step\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.97940\n",
            "328/328 - 17s - loss: 0.0685 - acc: 0.9749 - val_loss: 0.0864 - val_acc: 0.9734 - lr: 0.0010 - 17s/epoch - 53ms/step\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.97940\n",
            "328/328 - 17s - loss: 0.0718 - acc: 0.9730 - val_loss: 0.0787 - val_acc: 0.9725 - lr: 0.0010 - 17s/epoch - 53ms/step\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.97940\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "328/328 - 18s - loss: 0.0630 - acc: 0.9766 - val_loss: 0.0707 - val_acc: 0.9768 - lr: 0.0010 - 18s/epoch - 54ms/step\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 21: val_acc improved from 0.97940 to 0.98712, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 19s - loss: 0.0345 - acc: 0.9861 - val_loss: 0.0386 - val_acc: 0.9871 - lr: 1.0000e-04 - 19s/epoch - 58ms/step\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 22: val_acc improved from 0.98712 to 0.98798, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.0273 - acc: 0.9893 - val_loss: 0.0375 - val_acc: 0.9880 - lr: 1.0000e-04 - 18s/epoch - 55ms/step\n",
            "Epoch 23/1000\n",
            "\n",
            "Epoch 23: val_acc improved from 0.98798 to 0.98884, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.0245 - acc: 0.9914 - val_loss: 0.0402 - val_acc: 0.9888 - lr: 1.0000e-04 - 18s/epoch - 55ms/step\n",
            "Epoch 24/1000\n",
            "\n",
            "Epoch 24: val_acc improved from 0.98884 to 0.99056, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 19s - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0319 - val_acc: 0.9906 - lr: 1.0000e-04 - 19s/epoch - 57ms/step\n",
            "Epoch 25/1000\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.99056\n",
            "328/328 - 18s - loss: 0.0247 - acc: 0.9916 - val_loss: 0.0292 - val_acc: 0.9888 - lr: 1.0000e-04 - 18s/epoch - 54ms/step\n",
            "Epoch 26/1000\n",
            "\n",
            "Epoch 26: val_acc did not improve from 0.99056\n",
            "328/328 - 18s - loss: 0.0214 - acc: 0.9926 - val_loss: 0.0308 - val_acc: 0.9888 - lr: 1.0000e-04 - 18s/epoch - 54ms/step\n",
            "Epoch 27/1000\n",
            "\n",
            "Epoch 27: val_acc did not improve from 0.99056\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "328/328 - 18s - loss: 0.0234 - acc: 0.9917 - val_loss: 0.0352 - val_acc: 0.9906 - lr: 1.0000e-04 - 18s/epoch - 53ms/step\n",
            "Epoch 28/1000\n",
            "\n",
            "Epoch 28: val_acc improved from 0.99056 to 0.99142, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 19s - loss: 0.0247 - acc: 0.9918 - val_loss: 0.0267 - val_acc: 0.9914 - lr: 1.0000e-05 - 19s/epoch - 58ms/step\n",
            "Epoch 29/1000\n",
            "\n",
            "Epoch 29: val_acc did not improve from 0.99142\n",
            "328/328 - 18s - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0278 - val_acc: 0.9914 - lr: 1.0000e-05 - 18s/epoch - 54ms/step\n",
            "Epoch 30/1000\n",
            "\n",
            "Epoch 30: val_acc did not improve from 0.99142\n",
            "328/328 - 18s - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0271 - val_acc: 0.9914 - lr: 1.0000e-05 - 18s/epoch - 54ms/step\n",
            "Epoch 31/1000\n",
            "\n",
            "Epoch 31: val_acc improved from 0.99142 to 0.99227, saving model to ./models/train_ptbdb_cnn.h5\n",
            "328/328 - 18s - loss: 0.0180 - acc: 0.9933 - val_loss: 0.0265 - val_acc: 0.9923 - lr: 1.0000e-05 - 18s/epoch - 54ms/step\n",
            "Epoch 32/1000\n",
            "\n",
            "Epoch 32: val_acc did not improve from 0.99227\n",
            "328/328 - 19s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0282 - val_acc: 0.9906 - lr: 1.0000e-05 - 19s/epoch - 59ms/step\n",
            "Epoch 33/1000\n",
            "\n",
            "Epoch 33: val_acc did not improve from 0.99227\n",
            "328/328 - 18s - loss: 0.0190 - acc: 0.9935 - val_loss: 0.0269 - val_acc: 0.9914 - lr: 1.0000e-05 - 18s/epoch - 54ms/step\n",
            "Epoch 34/1000\n",
            "\n",
            "Epoch 34: val_acc did not improve from 0.99227\n",
            "\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "328/328 - 17s - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0259 - val_acc: 0.9923 - lr: 1.0000e-05 - 17s/epoch - 53ms/step\n",
            "Epoch 35/1000\n",
            "\n",
            "Epoch 35: val_acc did not improve from 0.99227\n",
            "328/328 - 18s - loss: 0.0172 - acc: 0.9940 - val_loss: 0.0262 - val_acc: 0.9923 - lr: 1.0000e-06 - 18s/epoch - 55ms/step\n",
            "Epoch 36/1000\n",
            "\n",
            "Epoch 36: val_acc did not improve from 0.99227\n",
            "328/328 - 19s - loss: 0.0175 - acc: 0.9941 - val_loss: 0.0261 - val_acc: 0.9923 - lr: 1.0000e-06 - 19s/epoch - 59ms/step\n",
            "Epoch 36: early stopping\n",
            "91/91 [==============================] - 1s 11ms/step\n",
            "Test f1 score : 0.9947743467933492 \n",
            "Test accuracy score : 0.992442459635864 \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras import optimizers, losses, activations, models\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_1 = pd.read_csv(\"./input/ptbdb_normal.csv\", header=None)\n",
        "df_2 = pd.read_csv(\"./input/ptbdb_abnormal.csv\", header=None)\n",
        "df = pd.concat([df_1, df_2])\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
        "\n",
        "Y = np.array(df_train[187].values).astype(np.int8)\n",
        "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
        "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "def get_model():\n",
        "    nclass = 1\n",
        "    #inp = Input(batch_shape=(1, 187, 1))\n",
        "    inp = Input(shape=(187, 1))\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = GlobalMaxPool1D()(img_1)\n",
        "    img_1 = Dropout(rate=0.2)(img_1)\n",
        "\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
        "    dense_1 = Dense(nclass, activation=activations.sigmoid, name=\"dense_3_ptbdb\")(dense_1)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=dense_1)\n",
        "    opt = optimizers.Adam(0.001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "file_path = \"./models/train_ptbdb_cnn.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
        "callbacks_list = [checkpoint, early, redonplat]  # early\n",
        "\n",
        "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
        "model.load_weights(file_path)\n",
        "\n",
        "pred_test = model.predict(X_test)\n",
        "pred_test = (pred_test>0.5).astype(np.int8)\n",
        "\n",
        "f1 = f1_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test accuracy score : %s \"% acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Transfer Learning từ MIT-BIH sang PTB-DB: Fine-tune toàn bộ mạng để phát hiện nhồi máu cơ tim\n",
        "Sử dụng biểu diễn đặc trưng đã học từ mô hình MIT-BIH và **huấn luyện lại toàn bộ mạng (fine-tuning)** với dữ liệu mới.\n"
      ],
      "metadata": {
        "id": "QCItSiz557jp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UmMtF_Wbv9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d797ee-fee8-43dc-a928-811aca047920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 187, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_16 (Conv1D)          (None, 183, 16)           96        \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 179, 16)           1296      \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPooling  (None, 89, 16)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 89, 16)            0         \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 87, 32)            1568      \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 85, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, 42, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 42, 32)            0         \n",
            "                                                                 \n",
            " conv1d_20 (Conv1D)          (None, 40, 32)            3104      \n",
            "                                                                 \n",
            " conv1d_21 (Conv1D)          (None, 38, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPooling  (None, 19, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 19, 32)            0         \n",
            "                                                                 \n",
            " conv1d_22 (Conv1D)          (None, 17, 256)           24832     \n",
            "                                                                 \n",
            " conv1d_23 (Conv1D)          (None, 15, 256)           196864    \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 256)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3_ptbdb (Dense)       (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 254,641\n",
            "Trainable params: 254,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_acc improved from -inf to 0.82575, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 16s - loss: 0.5076 - acc: 0.7500 - val_loss: 0.3968 - val_acc: 0.8258 - lr: 0.0010 - 16s/epoch - 50ms/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_acc improved from 0.82575 to 0.87811, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.3487 - acc: 0.8486 - val_loss: 0.2989 - val_acc: 0.8781 - lr: 0.0010 - 14s/epoch - 44ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_acc improved from 0.87811 to 0.88927, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.2887 - acc: 0.8778 - val_loss: 0.2541 - val_acc: 0.8893 - lr: 0.0010 - 14s/epoch - 44ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_acc improved from 0.88927 to 0.89700, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 15s - loss: 0.2487 - acc: 0.8979 - val_loss: 0.2469 - val_acc: 0.8970 - lr: 0.0010 - 15s/epoch - 45ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_acc improved from 0.89700 to 0.93562, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 15s - loss: 0.2270 - acc: 0.9119 - val_loss: 0.1905 - val_acc: 0.9356 - lr: 0.0010 - 15s/epoch - 45ms/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.93562\n",
            "328/328 - 14s - loss: 0.1852 - acc: 0.9272 - val_loss: 0.1656 - val_acc: 0.9339 - lr: 0.0010 - 14s/epoch - 43ms/step\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.93562\n",
            "328/328 - 14s - loss: 0.1680 - acc: 0.9338 - val_loss: 0.1822 - val_acc: 0.9288 - lr: 0.0010 - 14s/epoch - 42ms/step\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 8: val_acc improved from 0.93562 to 0.96395, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.1569 - acc: 0.9391 - val_loss: 0.1032 - val_acc: 0.9639 - lr: 0.0010 - 14s/epoch - 44ms/step\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.96395\n",
            "328/328 - 14s - loss: 0.1350 - acc: 0.9464 - val_loss: 0.0931 - val_acc: 0.9631 - lr: 0.0010 - 14s/epoch - 42ms/step\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.96395\n",
            "328/328 - 14s - loss: 0.1237 - acc: 0.9527 - val_loss: 0.0976 - val_acc: 0.9579 - lr: 0.0010 - 14s/epoch - 42ms/step\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 11: val_acc improved from 0.96395 to 0.96652, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 15s - loss: 0.1134 - acc: 0.9577 - val_loss: 0.0869 - val_acc: 0.9665 - lr: 0.0010 - 15s/epoch - 44ms/step\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 12: val_acc improved from 0.96652 to 0.96996, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.1023 - acc: 0.9608 - val_loss: 0.0700 - val_acc: 0.9700 - lr: 0.0010 - 14s/epoch - 43ms/step\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 13: val_acc improved from 0.96996 to 0.97682, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.0935 - acc: 0.9635 - val_loss: 0.0605 - val_acc: 0.9768 - lr: 0.0010 - 14s/epoch - 44ms/step\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.97682\n",
            "328/328 - 14s - loss: 0.0854 - acc: 0.9677 - val_loss: 0.0789 - val_acc: 0.9700 - lr: 0.0010 - 14s/epoch - 43ms/step\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 15: val_acc did not improve from 0.97682\n",
            "328/328 - 14s - loss: 0.0896 - acc: 0.9659 - val_loss: 0.0698 - val_acc: 0.9725 - lr: 0.0010 - 14s/epoch - 42ms/step\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 16: val_acc improved from 0.97682 to 0.98541, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.0740 - acc: 0.9729 - val_loss: 0.0498 - val_acc: 0.9854 - lr: 0.0010 - 14s/epoch - 44ms/step\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 17: val_acc did not improve from 0.98541\n",
            "328/328 - 14s - loss: 0.0701 - acc: 0.9725 - val_loss: 0.0844 - val_acc: 0.9700 - lr: 0.0010 - 14s/epoch - 41ms/step\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.98541\n",
            "328/328 - 14s - loss: 0.0683 - acc: 0.9733 - val_loss: 0.0690 - val_acc: 0.9785 - lr: 0.0010 - 14s/epoch - 41ms/step\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.98541\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "328/328 - 14s - loss: 0.0739 - acc: 0.9723 - val_loss: 0.0595 - val_acc: 0.9820 - lr: 0.0010 - 14s/epoch - 41ms/step\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 20: val_acc improved from 0.98541 to 0.98970, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.0499 - acc: 0.9829 - val_loss: 0.0356 - val_acc: 0.9897 - lr: 1.0000e-04 - 14s/epoch - 43ms/step\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 21: val_acc improved from 0.98970 to 0.99056, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.0359 - acc: 0.9866 - val_loss: 0.0325 - val_acc: 0.9906 - lr: 1.0000e-04 - 14s/epoch - 42ms/step\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 22: val_acc improved from 0.99056 to 0.99227, saving model to ./models/transfer_learning_ptbdb_full.h5\n",
            "328/328 - 14s - loss: 0.0327 - acc: 0.9881 - val_loss: 0.0316 - val_acc: 0.9923 - lr: 1.0000e-04 - 14s/epoch - 43ms/step\n",
            "Epoch 23/1000\n",
            "\n",
            "Epoch 23: val_acc did not improve from 0.99227\n",
            "328/328 - 13s - loss: 0.0345 - acc: 0.9876 - val_loss: 0.0344 - val_acc: 0.9888 - lr: 1.0000e-04 - 13s/epoch - 40ms/step\n",
            "Epoch 24/1000\n",
            "\n",
            "Epoch 24: val_acc did not improve from 0.99227\n",
            "328/328 - 13s - loss: 0.0322 - acc: 0.9890 - val_loss: 0.0326 - val_acc: 0.9880 - lr: 1.0000e-04 - 13s/epoch - 40ms/step\n",
            "Epoch 25/1000\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.99227\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "328/328 - 13s - loss: 0.0298 - acc: 0.9892 - val_loss: 0.0344 - val_acc: 0.9880 - lr: 1.0000e-04 - 13s/epoch - 40ms/step\n",
            "Epoch 26/1000\n",
            "\n",
            "Epoch 26: val_acc did not improve from 0.99227\n",
            "328/328 - 13s - loss: 0.0256 - acc: 0.9919 - val_loss: 0.0320 - val_acc: 0.9888 - lr: 1.0000e-05 - 13s/epoch - 40ms/step\n",
            "Epoch 27/1000\n",
            "\n",
            "Epoch 27: val_acc did not improve from 0.99227\n",
            "328/328 - 13s - loss: 0.0250 - acc: 0.9908 - val_loss: 0.0323 - val_acc: 0.9888 - lr: 1.0000e-05 - 13s/epoch - 39ms/step\n",
            "Epoch 27: early stopping\n",
            "91/91 [==============================] - 1s 6ms/step\n",
            "Test f1 score : 0.9933554817275747 \n",
            "Test accuracy score : 0.9903813122638269 \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import optimizers, losses, activations, models\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_1 = pd.read_csv(\"./input/ptbdb_normal.csv\", header=None)\n",
        "df_2 = pd.read_csv(\"./input/ptbdb_abnormal.csv\", header=None)\n",
        "df = pd.concat([df_1, df_2])\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
        "\n",
        "\n",
        "Y = np.array(df_train[187].values).astype(np.int8)\n",
        "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
        "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    nclass = 1\n",
        "    #inp = Input(batch_shape=(1, 187, 1))\n",
        "    inp = Input(shape=(187, 1))\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
        "    img_1 = GlobalMaxPool1D()(img_1)\n",
        "    img_1 = Dropout(rate=0.2)(img_1)\n",
        "\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
        "    dense_1 = Dense(nclass, activation=activations.sigmoid, name=\"dense_3_ptbdb\")(dense_1)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=dense_1)\n",
        "    opt = optimizers.Adam(0.001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "file_path = \"./models/transfer_learning_ptbdb_full.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
        "callbacks_list = [checkpoint, early, redonplat]  # early\n",
        "model.load_weights(\"./models/train_mitbih_cnn.h5\", by_name=True)\n",
        "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
        "model.load_weights(file_path)\n",
        "\n",
        "pred_test = model.predict(X_test)\n",
        "pred_test = (pred_test>0.5).astype(np.int8)\n",
        "\n",
        "f1 = f1_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test accuracy score : %s \"% acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Transfer Learning từ MIT-BIH sang PTB-DB: Freeze phần trích đặc trưng, huấn luyện lớp phân loại\n",
        "**Không huấn luyện lại (freeze) các lớp Conv1D** – tức giữ nguyên phần trích đặc trưng đã học từ bài toán phân loại nhịp tim.\n",
        "\n",
        "**Chỉ huấn luyện lại phần đầu ra (dense layers)** để thích ứng với bài toán mới: phân loại nhịp tim bình thường (0) và nhịp tim bất thường do nhồi máu cơ tim (1).\n"
      ],
      "metadata": {
        "id": "xhzh3dOr6AHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTvCUYs1bpIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d25ca4-7309-4c95-b7b8-7817bb4458f1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 187, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 183, 16)           96        \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 179, 16)           1296      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 89, 16)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 89, 16)            0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 87, 32)            1568      \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 85, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 42, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 42, 32)            0         \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 40, 32)            3104      \n",
            "                                                                 \n",
            " conv1d_13 (Conv1D)          (None, 38, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 19, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 19, 32)            0         \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 17, 256)           24832     \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 15, 256)           196864    \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3_ptbdb (Dense)       (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 254,641\n",
            "Trainable params: 20,673\n",
            "Non-trainable params: 233,968\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_acc improved from -inf to 0.69957, saving model to ./models/transfer_learning_ptbdb_freeze.h5\n",
            "328/328 - 4s - loss: 0.5838 - acc: 0.7244 - val_loss: 0.6062 - val_acc: 0.6996 - lr: 0.0010 - 4s/epoch - 13ms/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_acc did not improve from 0.69957\n",
            "328/328 - 3s - loss: 0.5685 - acc: 0.7243 - val_loss: 0.6253 - val_acc: 0.6910 - lr: 0.0010 - 3s/epoch - 9ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.69957\n",
            "328/328 - 4s - loss: 0.5572 - acc: 0.7216 - val_loss: 0.6276 - val_acc: 0.6884 - lr: 0.0010 - 4s/epoch - 12ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_acc did not improve from 0.69957\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "328/328 - 4s - loss: 0.5509 - acc: 0.7215 - val_loss: 0.6024 - val_acc: 0.6927 - lr: 0.0010 - 4s/epoch - 11ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_acc did not improve from 0.69957\n",
            "328/328 - 3s - loss: 0.5477 - acc: 0.7212 - val_loss: 0.6129 - val_acc: 0.6901 - lr: 1.0000e-04 - 3s/epoch - 9ms/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.69957\n",
            "328/328 - 3s - loss: 0.5447 - acc: 0.7214 - val_loss: 0.6167 - val_acc: 0.6884 - lr: 1.0000e-04 - 3s/epoch - 9ms/step\n",
            "Epoch 6: early stopping\n",
            "91/91 [==============================] - 1s 7ms/step\n",
            "Test f1 score : 0.8386195890684222 \n",
            "Test accuracy score : 0.7220886293369976 \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import optimizers, losses, activations, models\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_1 = pd.read_csv(\"./input/ptbdb_normal.csv\", header=None)\n",
        "df_2 = pd.read_csv(\"./input/ptbdb_abnormal.csv\", header=None)\n",
        "df = pd.concat([df_1, df_2])\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
        "\n",
        "\n",
        "Y = np.array(df_train[187].values).astype(np.int8)\n",
        "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
        "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    nclass = 1\n",
        "    inp = Input(shape=(187, 1))\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", trainable=False)(inp)\n",
        "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
        "    img_1 = Dropout(rate=0.1)(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\", trainable=False)(img_1)\n",
        "    img_1 = GlobalMaxPool1D()(img_1)\n",
        "    img_1 = Dropout(rate=0.2)(img_1)\n",
        "\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
        "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
        "    dense_1 = Dense(nclass, activation=activations.sigmoid, name=\"dense_3_ptbdb\")(dense_1)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=dense_1)\n",
        "    opt = optimizers.Adam(0.001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "file_path = \"./models/transfer_learning_ptbdb_freeze.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
        "callbacks_list = [checkpoint, early, redonplat]\n",
        "model.load_weights(\"./models/train_mitbih_cnn.h5\", by_name=True)\n",
        "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
        "model.load_weights(file_path)\n",
        "\n",
        "pred_test = model.predict(X_test)\n",
        "pred_test = (pred_test>0.5).astype(np.int8)\n",
        "\n",
        "f1 = f1_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "acc = accuracy_score(Y_test, pred_test)\n",
        "\n",
        "print(\"Test accuracy score : %s \"% acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7dMAvFhyKcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Convert to file .tflite"
      ],
      "metadata": {
        "id": "zgCgbXhzyMEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model('./models/transfer_learning_ptbdb_full.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('./models/train_mitbih_cnn.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Successfully converted to .tflite\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kB7mATgyh6E",
        "outputId": "027b258a-f7a1-48ff-9782-4cf10e94122a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp7oh5ohi8'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 187, 1), dtype=tf.float32, name='input_3')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136361725393232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361725393424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361725394768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726133840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726132304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726134608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726134032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726135376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726134800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726136144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726135568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726136912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726136336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726137680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726137104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726138448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726137872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726139216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726138640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726139984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726139408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136361726140752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ Đã chuyển thành công sang .tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Chuỗi dữ liệu bạn cung cấp\n",
        "raw_string = \"\"\"0.941 0.523 0.182 0.059 0.000 0.118 0.168 0.197 0.208 0.219 0.226 0.241 0.254 0.263 0.271 0.288 0.299 0.307 0.321 0.335 0.350 0.367 0.384 0.403 0.418 0.432 0.441 0.452 0.465 0.477 0.486 0.495 0.503 0.506 0.502 0.485 0.463 0.440 0.420 0.399 0.384 0.372 0.360 0.354 0.348 0.343 0.336 0.331 0.324 0.321 0.318 0.317 0.313 0.312 0.310 0.308 0.306 0.305 0.304 0.305 0.303 0.304 0.303 0.303 0.305 0.306 0.309 0.308 0.315 0.327 0.339 0.348 0.361 0.368 0.375 0.385 0.390 0.392 0.387 0.373 0.358 0.338 0.318 0.301 0.285 0.279 0.268 0.265 0.264 0.262 0.263 0.267 0.275 0.283 0.294 0.311 0.342 0.456 0.676 0.899 1.000 0.584 0.203 0.073 0.191 0.262 0.274 0.253 0.242 0.238 0.247 0.250 0.253 0.266 0.261 0.263 0.280 0.289 0.291 0.309 0.313 0.315 0.332 0.334 0.317 0.305 0.294 0.282 0.269 0.263 0.251 0.243 0.239 0.241 0.236 0.231 0.227 0.230 0.232 0.239 0.241 0.244 0.253 0.267 0.278 0.291 0.305 0.320 0.334 0.345 0.356 0.365 0.373 0.377 0.380 0.382 0.384 0.384 0.381 0.376 0.368 0.359 0.351 0.340 0.330 0.319 0.309 0.299 0.290 0.281 0.273 0.265 0.258 0.252 0.246 0.240 0.235 0.231 0.227 0.223 0.220 0.218 0.216 0.215 0.214 0.213 0.212\"\"\"\n",
        "\n",
        "# Parse và reshape thành đầu vào: (1, 187, 1)\n",
        "values = np.array([float(x) for x in raw_string.split()], dtype=np.float32)\n",
        "input_tensor = values.reshape(1, 187, 1)\n"
      ],
      "metadata": {
        "id": "KR0RlWq_EaN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"train_mitbih_cnn.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Lấy input/output\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Set input\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Lấy output\n",
        "output = interpreter.get_tensor(output_details[0]['index'])\n",
        "prob = float(output[0][0])\n",
        "label = int(prob > 0.5)\n",
        "\n",
        "print(f\"🔍 Abnormal Probability: {prob:.4f}\")\n",
        "print(f\"✅ Classification: {'ABNORMAL (1)' if label == 1 else 'NORMAL (0)'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9oY5I2EuXz",
        "outputId": "8134bcdc-a2e3-49a0-a6e6-4a4132a7ab76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Xác suất bất thường: 0.1094\n",
            "✅ Phân loại: BÌNH THƯỜNG (0)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "hfkzgT5K-iJO",
        "A004-3x4527r",
        "EAYCu_Yr55QF",
        "QCItSiz557jp",
        "xhzh3dOr6AHN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}